{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.preprocessing import image \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.image import rgb_to_grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN for Normal vs NonNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_nonnormal(x): \n",
    "    if x == 'Normal': \n",
    "        return x \n",
    "    else: \n",
    "        return 'Non-Normal'\n",
    "\n",
    "df = pd.read_csv('../CombinedImages/CombinedUpdated.csv')\n",
    "na_fill = {'VirusCategory1': 'Normal'}\n",
    "df = df.fillna(value = na_fill)\n",
    "\n",
    "df.VirusCategory1 = df.VirusCategory1.map(normal_nonnormal)\n",
    "df = df.join(pd.get_dummies(df.VirusCategory1.values, prefix = 'type'))\n",
    "columns_include = ['Normal']\n",
    "df = df[['ImagePath', 'VirusCategory1'] + [f'type_{i}' for i in columns_include]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    5841\n",
       "0     636\n",
       "Name: type_Normal, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.type_Normal.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imblearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7b98a1bff01e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'imblearn' is not defined"
     ]
    }
   ],
   "source": [
    "imblearn.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-96b1a6fbc99d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0moversampling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msmote\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "from imblearn import oversampling\n",
    "smote = SMOTE()\n",
    "\n",
    "\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['ImagePath', 'VirusCategory1']]\n",
    "y = df[[f'type_{i}' for i in columns_include]]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X,y, random_state = 10, stratify = X.VirusCategory1.values,\n",
    "                                                   train_size = .95)\n",
    "\n",
    "x_train = x_train.drop('VirusCategory1', axis = 1)\n",
    "x_test = x_test.drop('VirusCategory1', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(df[df['type_Normal']==1]), len(df[df['type_Normal']==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_value(path, img_type): \n",
    "    img = image.load_img(path, target_size = (28,28,3))\n",
    "    img = image.img_to_array(img)\n",
    "    if img_type == 'grey':\n",
    "        img = np.dot(img[...,:3], [.2989,.5870,.1140])\n",
    "    return img/255\n",
    "\n",
    "\n",
    "def get_data(df, img_type): \n",
    "    from tqdm import tqdm\n",
    "    img_list = [] \n",
    "    for path in tqdm(df.ImagePath.values):\n",
    "        path = f'../CombinedImages/all/{path}'\n",
    "        img_list.append(get_image_value(path, img_type)) \n",
    "    return np.array(img_list).squeeze()\n",
    "img_type = 'normal'\n",
    "x_test = get_data(x_test, img_type)\n",
    "x_train = get_data(x_train, img_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(x_test[0], cmap = plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "pickle.dump(x_train, open('picklesNorm/Binary_x_train_normal.p', 'wb'), protocol = 2)\n",
    "pickle.dump(x_test, open('picklesNorm/Binary_x_test_normal.p', 'wb'))\n",
    "\n",
    "# pickle.dump(x_train, open('picklesNorm/x_train_grey.p', 'wb'))\n",
    "# pickle.dump(x_test, open('picklesNorm/x_test_grey.p', 'wb'))\n",
    "\n",
    "pickle.dump(y_train, open('picklesNorm/Binary_y_train.p', 'wb'))\n",
    "pickle.dump(y_test, open('picklesNorm/Binary_y_test.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_value(path, img_type): \n",
    "    img = image.load_img(path, target_size = (28,28,3))\n",
    "    img = image.img_to_array(img)\n",
    "    if img_type == 'grey':\n",
    "        img = np.dot(img[...,:3], [.2989,.5870,.1140])\n",
    "    return img/255\n",
    "\n",
    "img_type = 'normal'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D \n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from keras.preprocessing import image \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tensorflow.image import rgb_to_grayscale\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_model_normal(img_type):\n",
    "    if img_type == 'normal': \n",
    "        inp_shape = (28,28,3)\n",
    "    else: \n",
    "        inp_shape = (28,28, 1)\n",
    "    act = 'sigmoid'\n",
    "    drop = .25 \n",
    "    kernal_reg = regularizers.l2(.001)\n",
    "    dil_rate = 2\n",
    "    adam = Adam(.001)\n",
    "    \n",
    "    model = Sequential() \n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3,3),activation=act, input_shape = inp_shape, \n",
    "                     kernel_regularizer = kernal_reg, kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation=act, kernel_regularizer = kernal_reg, dilation_rate = dil_rate, \n",
    "                     kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "   \n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(drop))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    return model \n",
    "# def get_conv_model_normal(img_type):\n",
    "#     if img_type == 'normal': \n",
    "#         inp_shape = (28,28,3)\n",
    "#     else: \n",
    "#         inp_shape = (28,28, 1)\n",
    "#     drop = .25 \n",
    "#     kernal_reg = regularizers.l2(.001)\n",
    "#     dil_rate = 2\n",
    "#     model = Sequential() \n",
    "    \n",
    "#     model.add(Conv2D(128, kernel_size=(3,3),activation='sigmoid', input_shape = inp_shape, \n",
    "#                      kernel_regularizer = kernal_reg, kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "\n",
    "#     model.add(Conv2D(128, (1, 1), activation='sigmoid', kernel_regularizer = kernal_reg, dilation_rate = dil_rate, \n",
    "#                      kernel_initializer = 'he_uniform', padding = 'same'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "# #     model.add(Conv2D(128, (1, 1), activation='relu', kernel_regularizer = kernal_reg, dilation_rate = dil_rate))\n",
    "# #     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dropout(drop))\n",
    "    \n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(drop))\n",
    "\n",
    "#     model.add(Dense(2, activation='sigmoid'))\n",
    "    \n",
    "#     model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "#     return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "\n",
    "def get_samples_norm(img_type): \n",
    "    x_train = pickle.load(open(f'picklesNorm/Binary_x_train_{img_type}.p', 'rb'))\n",
    "    x_test = pickle.load(open(f'picklesNorm/Binary_x_test_{img_type}.p', 'rb'))\n",
    "    y_train = pickle.load(open(f'picklesNorm/Binary_y_train.p', 'rb'))\n",
    "    y_test = pickle.load(open(f'picklesNorm/Binary_y_test.p', 'rb'))\n",
    "    return x_train, x_test, y_train, y_test\n",
    "x_train, x_test, y_train, y_test = get_samples_norm(img_type)\n",
    "print(x_train.shape)\n",
    "\n",
    "\n",
    "if img_type == 'grey': \n",
    "    x_train = x_train.reshape(len(x_train), 28, 28, 1)\n",
    "    x_test = x_test.reshape(len(x_test), 28, 28, 1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', verbose = 1, patience=5)\n",
    "model_checkpoint = ModelCheckpoint('models/Tester-NormalModelCheckpointWeights.h5', verbose = 1, save_best_only=True)\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "normal_model = get_conv_model_normal(img_type)\n",
    "normal_history = normal_model.fit(x_train, y_train, epochs = epochs, batch_size = batch_size, \n",
    "         callbacks = [early_stopping, model_checkpoint], validation_data = (x_test, y_test), verbose= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = normal_history.history['loss']\n",
    "train_acc = normal_history.history['accuracy']\n",
    "test_loss = normal_history.history['val_loss']\n",
    "test_acc = normal_history.history['val_accuracy']\n",
    "epochs = [i for i in range(1, len(test_acc)+1)]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize = (15,5))\n",
    "ax[0].plot(epochs, train_loss, label = 'Train Loss')\n",
    "ax[0].plot(epochs, test_loss, label = 'Test Loss')\n",
    "ax[0].set_title('Train/Test Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(epochs, train_acc, label = 'Train Accuracy')\n",
    "ax[1].plot(epochs, test_acc, label = 'Test Accuracy')\n",
    "ax[1].set_title('Train/Test Accuracy')\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \n",
    "        # Create the basic matrix\n",
    "#plt.figure(figsize = (10,10))\n",
    "\n",
    "    plt.imshow(cm,  cmap=plt.cm.Blues) \n",
    "    # Add title and axis labels\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Add appropriate axis scales\n",
    "    class_names = classes # Get class labels to add to matrix\n",
    "    tick_marks = np.arange(len(class_names))\n",
    "    plt.xticks(tick_marks, class_names, rotation=45)\n",
    "    plt.yticks(tick_marks, class_names)\n",
    "\n",
    "    # Add labels to each cell\n",
    "    thresh = cm.max() / 2. # Used for text coloring below\n",
    "    # Here we iterate through the confusion matrix and append labels to our visualization \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "            plt.text(j, i, cm[i, j],\n",
    "                     horizontalalignment='center',\n",
    "                     color='white' if cm[i, j] > thresh else 'black')\n",
    "\n",
    "    # Add a legend\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "normal_model = get_conv_model_normal(img_type)\n",
    "normal_model.load_weights('models/Tester-NormalModelCheckpointWeights.h5')\n",
    "\n",
    "y_test_predict = normal_model.predict(x_test).ravel()\n",
    "# print(y_test_predict) \n",
    "y_test_predict = np.where(y_test_predict >= .5, y_test_predict, 0)\n",
    "y_test_predict = np.where(y_test_predict <= .5, y_test_predict, 1)\n",
    "\n",
    "#y_test_predict = y_test_predict[y_test_predict > .5] =1 \n",
    "plot_confusion_matrix(confusion_matrix(y_test, y_test_predict), classes = [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# y_score = normal_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_model = get_conv_model_normal(img_type)\n",
    "normal_model.load_weights('models/Tester-NormalModelCheckpointWeights.h5')\n",
    "tester_img = get_image_value('TestImages/Normal6.jpg', img_type)\n",
    "if img_type == 'grey': \n",
    "    tester_img = np.reshape(tester_img, (1,28,28,1))\n",
    "else:\n",
    "    tester_img = np.reshape(tester_img, (1,28,28,3))\n",
    "normal_predict = normal_model.predict(tester_img).squeeze()\n",
    "print(normal_predict)\n",
    "if normal_predict < .5: \n",
    "    print('Non-Normal')\n",
    "else: \n",
    "    print('Normal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Random Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['Normal', 'COVID-19', 'Bacterial', 'Fungal', 'SARS']\n",
    "['Normal', 'COVID-19', 'Bacterial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
